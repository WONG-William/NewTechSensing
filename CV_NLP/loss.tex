\input{my_math_style}
\ifx\allfiles\undefined
\begin{document}
\fi
\section{Deep Learning Loss Function}
% ----------------------------------------------------------------------------
\begin{frame}
\frametitle{Deep Learning Loss Function}
	\small
	\begin{itemize}
		\item Data Loss\\
			\hspace{1cm}0-1 loss\\
			\hspace{1cm}log loss\\
			\hspace{1cm}hinge loss\\
			\hspace{1cm}squared loss\\
			\hspace{1cm}exponential loss
		\item Regulation Loss\\
			\hspace{1cm}$L_1$ norm\\
			\hspace{1cm}$L_2$ norm
	\end{itemize}
\end{frame}
% ----------------------------------------------------------------------------
\begin{frame}
\frametitle{Problem Definition}
	\small
	\begin{itemize}
		\item The loss function is used to evaluate the learning target, by deriving derivative and control iterative processes. \\
			\hspace{1cm}$L=L_{data}+L_{norm}$\\
			\hspace{1.5cm}$=\frac{1}{N}\sum_{i=0}^N L(y^{(i)},h_{\theta}(x^{(i)})) 
						+ \lambda L_{norm}$
		\item $L_{data}$ is data loss, to evaluate the classification.
		\item $L_{norm}$ is normalization loss, to evaluate the complexity of model to avoiding over-fit.
		\item actually $L$ has uniform form. Let $m_i=y^{(i)}\theta^Tx^{(i)}$, $L_i=L(m_i)$.\\
			\hspace{1cm}$L_{data}=\frac{1}{N}\sum_{i=0}^N L(m_i)$
		\item $h_{\theta^T}(x^{(i)})$ is called score function, comparing to hypothesis function of ML.\\
			\hspace{1cm} $L$ is called loss function, comparing to target function of ML.\\
			\hspace{1cm} Derivative is the same to ML.
	\end{itemize}
\end{frame}
% ----------------------------------------------------------------------------
\begin{frame}
\frametitle{Data Loss: 0-1 Loss}
	\small
	\begin{itemize}
		\item 0-1 loss: if $y^{(i)}$ and $\theta^Tx^{(i)}$ have the same sign, $L_i=0$.\\
			\hspace{1cm}if they have different sign, $L_i=1$, totally in the following equation:\\
				\begin{equation}
					L_{01}=
					\left\{
						\begin{aligned}
							0, if(m\geq 0)\\
							1, if(m < 0)
						\end{aligned}
					\right.
				\end{equation}
		\item Derivative is not analytic, so it is seldom used.
	\end{itemize}
\end{frame}
% ----------------------------------------------------------------------------
\begin{frame}
\frametitle{Data Loss: Log Loss}
	\small
	\begin{itemize}
		\item log loss
			$L_i=y^{(i)}\log h_{\theta}(x)+{(1-y^{(i)})}\log (1-h_{\theta}(x^{(i)}))$\\
				\hspace{1cm}$=y^{(i)}\log \frac{1}{1+e^{-\theta ^T x}}+{(1-y^{(i)})}\log (1-\frac{1}{1+e^{-\theta ^T x}})$\\
				\hspace{1cm}$=y^{(i)}\log \frac{1}{1+e^{-\theta ^T x}}+{(1-y^{(i)})}\log (\frac{1}{1+e^{\theta ^T x}})$\\
				\hspace{1cm}$=\frac{1}{1+e^{-\widetilde{y^{(i)}}\theta ^T x}}$
				\hspace{1cm} (when $y^{(i)}=0, \widetilde{y^{(i)}}=-1$; $y^{(i)}=1, \widetilde{y^{(i)}}=1$)	\\
				\hspace{1cm}$=\frac{1}{1+e^{-m_i}}$\\
				\hspace{1cm} softmax and logistic have the same expression.
		\item cross entropy to data loss: $H(p,q)=-\sum_x p(x)\log q(x)$.\\
			softmax	log loss function: 
			$L_{data}=-\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{k}1\{y_j^{(i)}=1\}\log({\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^{k} e^{\theta_l ^T x^{(i)}}}})$, with $p(x)=1\{y_j=1\}$ \\
		\item derivative\\
		\hspace{0cm}$=-\frac{1}{m}\sum_{i=1}^{m}
						(
							{1\{y_j^{(i)}=1\}}
								-\log (p(y^{(i)}=j|x^{(i)},\theta))
						){x^{(i)}}$\\
	\end{itemize}
\end{frame}
% ----------------------------------------------------------------------------
\begin{frame}
\frametitle{Data Loss: Hinge Loss}
	\small
	\begin{itemize}
		\item binary classification: $L_i=max(0,1-y^{(i)}\theta^Tx^{(i)})=max(0,1-m_i)$
		\\loss function: $L=L_{data}+\lambda L_{norm}=\frac{1}{m}\sum_{i=1}^{m}max(0,1-y^{(i)}\theta^Tx^{(i)})+\lambda L_{norm}$
		\item multiple classification:$L_i=\sum_{j\neq y^{(i)}}^{k}
										max(0,
											\theta_j^Tx^{(i)}-
											\theta_{y^{(i)}}^Tx^{(i)}+
											\Delta)$
		\\\hspace{3cm}or $L_i=			max(0,
											max_{j\neq y^{(i)}}^{k}(
											\theta_j^Tx^{(i)})-
											\theta_{y^{(i)}}^Tx^{(i)}+
											1)$
		\item derivative\\
\footnotesize
				\begin{equation}
					\nabla_{\theta_j} L=
					\left\{
						\begin{aligned}
							1(
								\theta_j^Tx^{(i)}
								-\theta_{y^{(i)}}^Tx^{(i)}
								+\Delta > 0
							)x^{(i)}, if(j\neq y^{(i)})\\
							-(
								\sum_{j\neq y^{(i)}}^k
									1(
										\theta_j^Tx^{(i)}
										-\theta_{y^{(i)}}^Tx^{(i)}
										+\Delta > 0
									)
							)x^{(i)}, if(j= y^{(i)})
						\end{aligned}
					\right.
				\end{equation}
				\begin{equation}
					\nabla_{\theta_j} L=
					\left\{
						\begin{aligned}
							1(
										\theta_j^Tx^{(i)}-
										\theta_{y^{(i)}}^Tx^{(i)}+
										1 > 0
							)x^{(i)}, 
								if(j\neq y^{(i)} and \theta_j^Tx^{(i)} is max)\\
							0, if(j\neq y^{(i)} and \theta_j^Tx^{(i)} is not max)\\
							-(
								\sum_{j\neq y^{(i)}}^k
									1(
											\theta_j^Tx^{(i)}-
											\theta_{y^{(i)}}^Tx^{(i)}+
											1 > 0
									)
							)x^{(i)}, if(j= y^{(i)})
						\end{aligned}
					\right.
				\end{equation}
	\end{itemize}
\end{frame}
% ----------------------------------------------------------------------------
\begin{frame}
\frametitle{Data Loss: Squared Loss and Exponential Loss}
	\small
	\begin{itemize}
		\item squared loss is for linear regression, $L_i=(y^{(i)}-\theta^Tx^{(i)})^2$
		\hspace{1cm} derivative:$\nabla_{\theta_j} L=x^{(i)}$
		\item exponential loss for boosting algorithm, $L_i=exp(-y^{(i)}\theta^Tx^{(i)})=exp(-m_i)$
		\hspace{1cm} derivative:
		\item all loss function is decreased by increasing x, except squared loss and 0-1 loss.
	\end{itemize}
\end{frame}
% ----------------------------------------------------------------------------
\begin{frame}
\frametitle{Normalization Loss}
	\small
	\begin{itemize}
	\item $L_{norm}=L_p=(\sum_i|\theta_i|^p)^{\frac{1}{p}}$
	\item $L_1=\sum_i|\theta_i|$
	\item $L_2=(\sum_i|\theta_i|^2)^{\frac{1}{2}}$, which is the most widely used.
	\item $L_{norm}$ should be added into the calculation of derivatives.
	\end{itemize}
\end{frame}
% ----------------------------------------------------------------------------
\ifx\allfiles\undefined
\end{document}
\fi
